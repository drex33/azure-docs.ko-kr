### YamlMime:FAQ
metadata:
  title: IoT Edge의 Azure Live Video Analytics FAQ
  description: 이 문서는 IoT Edge의 Azure Live Video Analytics에 대한 일반적인 질문과 대답입니다.
  ms.topic: article
  ms.service: media-services
  ms.date: 12/01/2020
  ms.openlocfilehash: 068714c46805bb65affa292284126aab0e41098b
  ms.sourcegitcommit: 05dd6452632e00645ec0716a5943c7ac6c9bec7c
  ms.translationtype: HT
  ms.contentlocale: ko-KR
  ms.lasthandoff: 08/17/2021
  ms.locfileid: "122252939"
title: IoT Edge의 Azure Live Video Analytics FAQ
summary: "\n[!INCLUDE [redirect to Azure Video Analyzer](./includes/redirect-video-analyzer.md)]\n\n이 문서는 Azure IoT Edge의 Live Video Analytics에 대한 일반적인 질문과 대답입니다.\n"
sections:
- name: 일반
  questions:
  - question: >
      그래프 토폴로지 정의에서 사용할 수 있는 시스템 변수는 무엇인가요?
    answer: "| 변수   |  설명  | \n| --- | --- | \n| [System.DateTime](/dotnet/framework/data/adonet/sql/linq/system-datetime-methods) | 일반적으로 다음 형식의 날짜와 시간으로 표현되는 UTC 시간의 한 순간을 나타냅니다.<br>*yyyyMMddTHHmmssZ* | \n| System.PreciseDateTime | UTC(협정 세계시) 날짜/시간 인스턴스를 다음과 같은 밀리초 단위의 ISO8601 파일 호환 형식으로 나타냅니다.<br>*yyyyMMddTHHmmss.fffZ* | \n| System.GraphTopologyName   | 미디어 그래프 토폴로지를 나타내며, 그래프의 청사진을 보유합니다. | \n| System.GraphInstanceName |    미디어 그래프 인스턴스를 나타내며, 매개 변수 값을 보유하고, 토폴로지를 참조합니다. | \n"
- name: 구성 및 배포
  questions:
  - question: >
      미디어 에지 모듈을 Windows 10 디바이스에 배포할 수 있나요?
    answer: >
      예. 자세한 내용은 [Windows 10의 Linux 컨테이너](/virtualization/windowscontainers/deploy-containers/linux-containers)를 참조하세요.
- name: IP 카메라 및 RTSP 설정에서 캡처
  questions:
  - question: >
      비디오 스트림으로 보내려면 디바이스에서 특별한 SDK를 사용해야 하나요?
    answer: >
      아니요, IoT Edge의 Live Video Analytics는 RTSP(실시간 스트리밍 프로토콜)를 대부분의 IP 카메라에서 지원되는 비디오 스트리밍에 용하여 미디어를 캡처할 수 있습니다.
  - question: >
      RTMP(실시간 메시징 프로토콜) 또는 부드러운 스트리밍 프로토콜(예: Media Services 라이브 이벤트)을 사용하여 미디어를 IoT Edge의 Live Video Analytics에 푸시할 수 있나요?
    answer: "아니요, Live Video Analytics는 IP 카메라에서 비디오를 캡처하는 데 RTSP만 지원합니다. TCP/HTTP를 통한 RTSP 스트리밍을 지원하는 모든 카메라가 작동합니다. \n"
  - question: >
      그래프 인스턴스에서 RTSP 원본 URL을 다시 설정하거나 업데이트할 수 있나요?
    answer: "예, 그래프 인스턴스가 *비활성* 상태인 경우에 가능합니다.  \n"
  - question: >
      테스트 및 개발 중에 RTSP 시뮬레이터를 사용할 수 있나요?
    answer: >
      예, [RTSP 시뮬레이터](https://github.com/Azure/live-video-analytics/tree/master/utilities/rtspsim-live555) 에지 모듈은 학습 프로세스를 지원하는 빠른 시작 및 자습서에서 사용할 수 있습니다. 이 모듈은 최선의 노력으로 제공되며 항상 사용할 수 있는 것은 아닙니다. 시뮬레이터를 몇 시간 이상 사용하지 *않는* 것이 좋습니다. 프로덕션 배포를 계획하기 전에 실제 RTSP 원본을 사용하여 테스트하는 데 투자해야 합니다.
  - question: >
      Edge에서 IP 카메라의 ONVIF 검색을 지원하나요?
    answer: >
      아니요. 에지의 디바이스에 대한 ONVIF(오픈 네트워크 비디오 인터페이스 포럼) 검색을 지원하지 않습니다.
- name: 스트리밍 및 재생
  questions:
  - question: >
      HLS 또는 DASH와 같은 스트리밍 기술을 사용하여 Azure Media Services에 기록된 자산을 에지에서 재생할 수 있나요?
    answer: >
      예. Azure Media Services의 다른 자산과 마찬가지로 기록된 자산을 스트림할 수 있습니다. 콘텐츠를 스트림하려면 스트리밍 엔드포인트가 만들어져 실행 중 상태여야 합니다. 표준 스트리밍 로케이터 만들기 프로세스를 사용하면 사용 가능한 플레이어 프레임워크로 스트림하기 위해 Apple HLS(HTTP 라이브 스트리밍) 또는 DASH(Dynamic Adaptive Streaming over HTTP, MPEG-DASH라고도 함) 매니페스트에 액세스할 수 있습니다. HLS 또는 DASH 매니페스트를 만들고 게시하는 방법에 대한 자세한 내용은 [동적 패키징](../latest/encode-dynamic-packaging-concept.md)을 참조하세요.
  - question: >
      보관된 자산에서 Media Services의 표준 콘텐츠 보호 및 DRM 기능을 사용할 수 있나요?
    answer: >
      예. 모든 표준 동적 암호화 콘텐츠 보호 및 DRM(디지털 권한 관리) 기능은 미디어 그래프에서 기록된 자산에 사용할 수 있습니다.
  - question: >
      기록된 자산의 콘텐츠를 보는 데 사용할 수 있는 플레이어는 무엇인가요?
    answer: >
      규격 HLS 버전 3 또는 버전 4를 지원하는 모든 표준 플레이어가 지원됩니다. 규격 MPEG-DASH 재생이 가능한 플레이어도 지원됩니다.


      테스트용으로 권장되는 플레이어는 다음과 같습니다.


      * [Azure Media Player](../latest/player-use-azure-media-player-how-to.md)

      * [HLS.js](https://hls-js.netlify.app/demo/)

      * [Video.js](https://videojs.com/)

      * [Dash.js](https://github.com/Dash-Industry-Forum/dash.js/wiki)

      * [Shaka Player](https://github.com/google/shaka-player)

      * [ExoPlayer](https://github.com/google/ExoPlayer)

      * [Apple 네이티브 HTTP 라이브 스트리밍](https://developer.apple.com/streaming/)

      * Edge, Chrome 또는 Safari 기본 제공 HTML5 비디오 플레이어

      * HLS 또는 DASH 재생을 지원하는 상업용 플레이어
  - question: >
      미디어 그래프 자산 스트리밍에 대한 제한 사항은 무엇인가요?
    answer: >
      미디어 그래프에서 라이브 또는 기록된 자산을 스트림하려면 Media Services에서 미디어 및 엔터테인먼트, OTT(Over the Top) 및 브로드캐스트 고객의 주문형 및 라이브 스트리밍에 지원하는 것과 동일한 대규모 인프라 및 스트리밍 엔드포인트를 사용합니다. 즉, Azure Content Delivery Network, Verizon 또는 Akamai를 사용하도록 설정하여 시나리오에 따라 적은 수에서 최대 수백만 명에 이르는 시청자에게 콘텐츠를 빠르고 쉽게 제공할 수 있습니다.


      콘텐츠는 Apple HLS 또는 MPEG-DASH를 사용하여 전송할 수 있습니다.
- name: AI 모델 디자인
  questions:
  - question: >
      Docker 컨테이너에 래핑된 여러 AI 모델이 있습니다. Live Video Analytics에서 이러한 모델을 어떻게 사용해야 하나요?
    answer: "솔루션은 Live Video Analytics와 통신하기 위해 추론 서버에서 사용하는 통신 프로토콜에 따라 달라집니다. 다음 섹션에서는 각 프로토콜의 작동 방식에 대해 설명합니다.\n\n*HTTP 프로토콜 사용*:\n\n* 단일 컨테이너(단일 lvaExtension):  \n\n   추론 서버에서 단일 포트를 사용할 수 있지만, 서로 다른 엔드포인트를 여러 AI 모델에 사용할 수 있습니다. 예를 들어 Python 샘플의 경우 다음과 같이 모델별로 서로 다른 `route`를 사용할 수 있습니다. \n\n   ```\n   @app.route('/score/face_detection', methods=['POST']) \n   … \n   Your code specific to face detection model… \n\n   @app.route('/score/vehicle_detection', methods=['POST']) \n   … \n   Your code specific to vehicle detection model \n   … \n   ```\n\n   그런 다음, Live Video Analytics 배포에서 그래프를 인스턴스화할 때 각 인스턴스에 대한 추론 서버 URL을 다음과 같이 설정합니다. \n\n   첫 번째 인스턴스: 추론 서버 URL= `http://lvaExtension:44000/score/face_detection`<br/>\n   두 번째 인스턴스: 추론 서버 URL = `http://lvaExtension:44000/score/vehicle_detection`  \n   \n    > [!NOTE]\n    > 또는 그래프를 인스턴스화할 때 AI 모델을 다른 포트에 공개하고 호출할 수 있습니다.  \n\n* 여러 컨테이너: \n\n   각 컨테이너는 다른 이름으로 배포됩니다. 이전에는 Live Video Analytics 설명서 세트에서 *lvaExtension* 이라는 확장을 배포하는 방법을 보여 주었습니다. 이제 각각 동일한 HTTP 인터페이스를 사용하는 두 개의 서로 다른 컨테이너를 개발할 수 있습니다. 즉, 동일한 `/score` 엔드포인트가 있습니다. 이러한 두 컨테이너를 다른 이름으로 배포하고, 모두 *서로 다른 포트* 에서 수신 대기하는지 확인합니다. \n\n   예를 들어 `lvaExtension1`이라는 컨테이너는 `44000` 포트를 수신 대기하고, `lvaExtension2`라는 두 번째 컨테이너는 `44001` 포트를 수신 대기합니다. \n\n   Live Video Analytics 토폴로지에서 다음과 같이 서로 다른 추론 URL을 사용하여 두 개의 그래프를 인스턴스화합니다. \n\n   첫 번째 인스턴스: 추론 서버 URL = `http://lvaExtension1:44001/score`    \n   두 번째 인스턴스: 추론 서버 URL = `http://lvaExtension2:44001/score`\n   \n*gRPC 프로토콜 사용*: \n\n* Live Video Analytics 모듈 1.0에서는 gRPC 서버에서 다른 포트를 통해 다른 AI 모델을 공개하는 경우에만 gRPC(범용 원격 프로 시저 호출) 프로토콜을 사용할 할 수 있습니다. [이 코드 예제](https://github.com/Azure/live-video-analytics/blob/master/MediaGraph/topologies/grpcExtensionOpenVINO/2.0/topology.json)에서 단일 44000 포트는 모든 yolo 모델을 공개합니다. 이론적으로 yolo gRPC 서버는 44000 포트에서 일부 모델을 공개하고, 45000 포트에서 다른 모델을 공개하도록 다시 작성할 수 있습니다. \n\n* Live Video Analytics 모듈 2.0에서는 새 속성이 gRPC 확장 노드에 추가됩니다. 이 **extensionConfiguration** 속성은 gRPC 계약의 일부로 사용할 수 있는 선택적 문자열입니다. 여러 AI 모델이 하나의 추론 서버에 패키지된 경우 모든 AI 모델에 대한 노드를 공개할 필요가 없습니다. 대신 그래프 인스턴스의 경우 확장 공급자는 **extensionConfiguration** 속성을 사용하여 여러 AI 모델을 선택하는 방법을 정의할 수 있습니다. Live Video Analytics는 실행 중에 이 문자열을 추론 서버에 전달하여 원하는 AI 모델을 호출하는 데 사용할 수 있습니다. \n"
  - question: >
      AI 모델을 중심으로 gRPC 서버를 구축하고 있으며, 여러 카메라 또는 그래프 인스턴스에서 이 서버를 사용할 수 있도록 지원하려고 합니다. 서버를 구축하려면 어떻게 해야 하나요?
    answer: "먼저 서버에서 둘 이상의 요청을 한 번에 처리하거나 병렬 스레드에서 작업을 수행할 수 있어야 합니다. \n\n예를 들어 다음 [Live Video Analytics gRPC 샘플](https://github.com/Azure/live-video-analytics/blob/master/utilities/video-analysis/notebooks/Yolo/yolov3/yolov3-grpc-icpu-onnx/lvaextension/server/server.py)에는 기본 병렬 채널 수가 설정되어 있습니다. \n\n```\nserver = grpc.server(futures.ThreadPoolExecutor(max_workers=3)) \n```\n\n이전의 gRPC 서버 인스턴스화에서 서버는 카메라당 또는 그래프 토폴로지 인스턴스당 세 개의 채널만 한 번에 열 수 있습니다. 세 개가 넘는 인스턴스는 서버에 연결하지 마세요. 세 개가 넘는 채널을 열려고 하면 기존 채널이 삭제될 때까지 요청이 보류됩니다.  \n\n이전의 gRPC 서버 구현은 Python 샘플에서 사용됩니다. 개발자는 자신의 고유한 서버를 구현하거나 이전 기본 구현을 사용하여 작업자 수를 늘릴 수 있으며, 이를 비디오 피드에 사용할 카메라 수로 설정합니다. \n\n여러 카메라를 설정하고 사용하려면 각각 동일하거나 다른 추론 서버(예: 이전 단락에서 언급한 서버)를 가리키는 여러 개의 그래프 토폴로지 인스턴스를 인스턴스화할 수 있습니다. \n"
  - question: >
      추론 의사 결정을 내리기 전에 업스트림에서 여러 프레임을 받을 수 있기를 원합니다. 이 기능을 사용하도록 설정하려면 어떻게 해야 하나요?
    answer: "현재 [기본 샘플](https://github.com/Azure/live-video-analytics/tree/master/utilities/video-analysis)은 *상태 비저장* 모드에서 작동합니다. 이전 호출의 상태, 심지어 호출한 사용자의 상태도 유지하지 않습니다. 즉, 여러 토폴로지 인스턴스에서 동일한 추론 서버를 호출할 수 있지만 해당 서버에서 호출자 또는 호출자별 상태를 구분할 수 없습니다. \n\n*HTTP 프로토콜 사용*:\n\n상태를 유지하기 위해 각 호출자 또는 그래프 토폴로지 인스턴스는 호출자 고유의 HTTP 쿼리 매개 변수를 사용하여 추론 서버를 호출합니다. 예를 들어 각 인스턴스의 추론 서버 URL 주소는 다음과 같습니다.  \n\n첫 번째 토폴로지 인스턴스 = `http://lvaExtension:44000/score?id=1`<br/>\n두 번째 토폴로지 인스턴스 = `http://lvaExtension:44000/score?id=2`\n          \n서버 쪽에서 점수 경로는 호출자를 인식합니다. ID가 1이면 해당 호출자 또는 그래프 토폴로지 인스턴스에 대한 상태를 별도로 유지할 수 있습니다. 그런 다음, 받은 비디오 프레임을 버퍼에 유지할 수 있습니다. 예를 들어 배열 또는 DateTime 키가 있는 사전을 사용하고, 값은 프레임입니다. 그런 다음, *x* 개의 프레임을 받은 후에 처리(추론)할 서버를 정의할 수 있습니다. \n\n*gRPC 프로토콜 사용*: \n\ngRPC 확장을 사용하면 각 세션이 단일 카메라 피드용이므로 ID를 제공할 필요가 없습니다. 이제 extensionConfiguration 속성을 사용하여 비디오 프레임을 버퍼에 저장하고, *x* 개의 프레임을 받은 후에 처리(추론)할 서버를 정의할 수 있습니다. \n"
  - question: >
      특정 컨테이너의 모든 ProcessMediaStreams에서 동일한 AI 모델을 실행하나요?
    answer: "아니요. 그래프 인스턴스에서 최종 사용자의 시작 또는 중지 호출은 세션을 구성하거나 카메라 연결을 끊거나 다시 연결하는 것일 수도 있습니다. 이는 카메라에서 비디오를 스트림하는 경우 하나의 세션을 유지하기 위한 것입니다. \n\n* 처리를 위해 비디오를 보내는 두 개의 카메라에서 두 개의 세션을 만듭니다. \n* 두 개의 gRPC 확장 노드가 있는 그래프로 이동하는 하나의 카메라에서 두 개의 세션을 만듭니다. \n\n각 세션은 Live Video Analytics와 gRPC 서버 간의 전이중 연결이며, 각 세션마다 다른 모델 또는 파이프라인이 있을 수 있습니다. \n\n> [!NOTE]\n> 카메라의 연결을 끊거나 다시 연결하는 경우 허용 제한을 초과하는 기간 동안 카메라가 오프라인 상태가 되면 Live Video Analytics에서 gRPC 서버를 사용하여 세션을 엽니다. 서버에서 이러한 세션의 상태를 추적할 필요가 없습니다. \n\n또한 Live Video Analytics는 그래프 인스턴스의 단일 카메라에 대한 여러 gRPC 확장에 대한 지원을 추가합니다. 이러한 gRPC 확장을 사용하여 AI 처리를 순차적으로, 병렬로 또는 둘 모두를 조합하여 수행할 수 있습니다. \n\n> [!NOTE]\n> 여러 확장을 병렬로 실행하면 하드웨어 리소스에 영향을 줍니다. 컴퓨팅 요구 사항에 맞는 하드웨어를 선택하는 경우 이를 고려해야 합니다. \n"
  - question: >
      동시 ProcessMediaStreams의 최대 수는 얼마인가요?
    answer: "Live Video Analytics는 이 수를 제한하지 않습니다.  \n"
  - question: >
      추론 서버에서 CPU 또는 GPU 또는 다른 하드웨어 가속기를 사용해야 하는지 여부를 결정하려면 어떻게 해야 하나요?
    answer: "결정은 개발된 AI 모델의 복잡성과 CPU 및 하드웨어 가속기를 사용하는 방법에 따라 달라집니다. AI 모델을 개발할 때 모델에서 사용해야 하는 리소스와 수행해야 하는 작업을 지정할 수 있습니다. \n"
  - question: >
      경계 상자 후 처리를 사용하여 이미지를 저장하려면 어떻게 할까요?
    answer: "현재 경계 상자 좌표는 추론 메시지로만 제공하고 있습니다. 이러한 메시지를 사용하고 비디오 프레임에서 경계 상자를 오버레이할 수 있는 사용자 지정 MJPEG 스트리머를 빌드할 수 있습니다.  \n"
- name: gRPC 호환성
  questions:
  - question: >
      미디어 스트림 설명자에 대한 필수 필드는 어떻게 알 수 있나요?
    answer: "값을 제공하지 않는 필드에는 [gRPC에서 지정한 대로 기본값](https://developers.google.com/protocol-buffers/docs/proto3#default)이 지정됩니다.  \n\nLive Video Analytics는 *proto3* 버전의 프로토콜 버퍼 언어를 사용합니다. Live Video Analytics 계약에서 사용하는 모든 프로토콜 버퍼 데이터는 [프로토콜 버퍼 파일](https://github.com/Azure/live-video-analytics/tree/master/contracts/grpc)에서 사용할 수 있습니다. \n"
  - question: >
      최신 프로토콜 버퍼 파일을 사용하는지 확인하려면 어떻게 해야 하나요?
    answer: "[계약 파일 사이트](https://github.com/Azure/live-video-analytics/tree/master/contracts/grpc)에서 최신 프로토콜 버퍼 파일을 가져올 수 있습니다. 계약 파일을 업데이트할 때마다 이 위치에 있습니다. 프로토콜 파일을 즉시 업데이트할 계획이 없으므로 파일의 위쪽에 있는 패키지 이름을 찾아서 버전을 확인합니다. 이 단계는 다음과 같아야 합니다. \n\n```\nmicrosoft.azure.media.live_video_analytics.extensibility.grpc.v1 \n```\n\n이러한 파일을 업데이트하면 이름 끝에 있는 \"v-값\"이 증가합니다. \n\n> [!NOTE]\n> Live Video Analytics에서 proto3 버전의 언어를 사용하므로 필드는 선택 사항이며 버전은 이전 버전 및 이후 버전과 호환됩니다. \n"
  - question: >
      Live Video Analytics와 함께 사용할 수 있는 gRPC 기능은 무엇인가요? 필수 기능과 선택 기능은 무엇인가요?
    answer: "프로토콜 버퍼(Protobuf) 계약이 충족되는 경우 모든 서버 쪽 gRPC 기능을 사용할 수 있습니다. \n"
- name: 모니터링 및 메트릭
  questions:
  - question: >
      Azure Event Grid를 사용하여 에지에서 미디어 그래프를 모니터링할 수 있나요?
    answer: "예. Prometheus 메트릭을 사용하고, 이벤트 그리드에 게시할 수 있습니다. \n"
  - question: >
      Azure Monitor를 사용하여 클라우드 또는 에지에서 미디어 그래프의 상태, 메트릭 및 성능을 볼 수 있나요?
    answer: >
      예, 이 방법을 지원합니다. 자세한 내용은 [Azure Monitor 메트릭 개요](../../azure-monitor/essentials/data-platform-metrics.md)를 참조하세요.
  - question: >
      Media Services IoT Edge 모듈을 더 쉽게 모니터링할 수 있는 도구가 있나요?
    answer: "Visual Studio Code는 LVAEdge 모듈 엔드포인트를 쉽게 모니터링할 수 있는 Azure IoT Tools 확장을 지원합니다. 이 도구를 사용하여 \"이벤트\"에 대한 IoT 허브 기본 제공 엔드포인트 모니터링을 빠르게 시작하고, 에지 디바이스에서 클라우드로 라우팅되는 추론 메시지를 볼 수 있습니다. \n\n또한 이 확장을 사용하여 LVAEdge 모듈의 모듈 쌍을 편집하여 미디어 그래프 설정을 수정할 수 있습니다.\n\n자세한 내용은 [모니터링 및 로깅 문서](monitoring-logging.md)를 참조하세요.\n"
- name: 요금 청구 및 가용성
  questions:
  - question: >
      IoT Edge의 Live Video Analytics에 대한 요금을 청구하는 방법은 어떻게 되나요?
    answer: >
      청구에 대한 자세한 내용은 [Media Services 가격](https://azure.microsoft.com/pricing/details/media-services/)을 참조하세요.
additionalContent: "\n## <a name=\"next-steps\"></a>다음 단계\n\n[빠른 시작: IoT Edge의 Live Video Analytics 시작](get-started-detect-motion-emit-events-quickstart.md)"
