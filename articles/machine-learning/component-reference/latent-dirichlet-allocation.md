---
title: '잠재적 Dirichlet 할당: 구성 요소 참조'
titleSuffix: Azure Machine Learning
description: Latent Dirichlet 할당 구성 요소를 사용하여 분류되지 않은 텍스트를 범주로 그룹화하는 방법을 알아봅니다.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: reference
author: likebupt
ms.author: keli19
ms.date: 06/05/2020
ms.openlocfilehash: f8256a6cfd3b59f6f47cc703d05451ea594885cb
ms.sourcegitcommit: e41827d894a4aa12cbff62c51393dfc236297e10
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 11/04/2021
ms.locfileid: "131570060"
---
# <a name="latent-dirichlet-allocation-component"></a>잠재적 Dirichlet 할당 구성 요소

이 문서에서는 Azure Machine Learning 디자이너에서 Latent Dirichlet 할당 구성 요소를 사용하여 분류되지 않은 텍스트를 범주로 그룹화하는 방법을 설명합니다. 

LDA(잠재적 Dirichlet 할당)는 유사한 텍스트를 찾기 위해 자연어 처리에서 자주 사용됩니다. 또 다른 일반적인 용어는 ‘토픽 모델링’입니다.

이 구성 요소는 텍스트 열을 가져와서 다음과 같은 출력을 생성합니다.

+ 원본 텍스트 및 각 범주의 점수

+ 각 범주의 추출된 용어와 계수가 포함된 기능 매트릭스

+ 저장하고 입력으로 사용된 새 텍스트에 다시 적용할 수 있는 변환

이 구성 요소는 scikit-learn 라이브러리를 사용합니다. scikit-learn에 대한 자세한 내용은 알고리즘에 대한 설명과 자습서가 포함된 [GitHub 리포지토리](https://github.com/scikit-learn/scikit-learn)를 참조하세요.

## <a name="more-about-latent-dirichlet-allocation"></a>잠재적 Dirichlet 할당에 대한 자세한 정보

일반적으로 LDA는 분류 방법이 아닙니다. 그러나 생성 방법을 사용하므로 알려진 클래스 레이블을 제공하고 패턴을 유추할 필요가 없습니다.  대신, 알고리즘은 토픽 그룹을 식별하는 데 사용되는 확률 모델을 생성합니다. 확률 모델을 사용하여 모델에 입력으로 제공하는 기존 학습 사례나 새 사례를 분류할 수 있습니다.

텍스트와 범주 간 관계를 강력하게 가정하지 않는 생성 모델을 선호할 수도 있습니다. 생성 모델은 단어 분포만 사용하여 수학적으로 토픽을 모델링합니다.

이론은 PDF 다운로드로 제공되는 [잠재적 Dirichlet 할당: Blei, Ng, Jordan](https://ai.stanford.edu/~ang/papers/nips01-lda.pdf) 문서에 설명되어 있습니다.

이 구성 요소의 구현은 LDA용 [scikit-learn 라이브러리를](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/_lda.py) 기반으로 합니다.

자세한 내용은 [기술 참고 사항](#technical-notes) 섹션을 참조하세요.

## <a name="how-to-configure-latent-dirichlet-allocation"></a>잠재적 Dirichlet 할당을 구성하는 방법

이 구성 요소에는 원시 또는 전처리된 텍스트 열이 포함된 데이터 세트가 필요합니다.

1. **파이프라인에 Latent Dirichlet 할당** 구성 요소를 추가합니다.

2. 구성 요소에 대한 입력으로 하나 이상의 텍스트 열이 포함된 데이터 세트를 제공합니다.

3. **대상 열** 에서 분석할 텍스트가 포함된 열을 하나 이상 선택합니다.

    여러 열을 선택할 수 있지만 **문자열** 데이터 형식이어야 합니다.

    LDA는 텍스트에서 큰 기능 매트릭스를 만들기 때문에 일반적으로 단일 텍스트 열을 분석합니다.

4. **모델링할 토픽 수** 에 입력 텍스트에서 파생시킬 범주 또는 토픽 수를 나타내는 1에서 1000 사이의 정수를 입력합니다.

    기본적으로 5개 토픽이 생성됩니다.

5. **N-Gram** 에서 해시 중에 생성되는 N-Gram의 최대 길이를 지정합니다.

    기본값은 2로, bigram과 unigram이 둘 다 생성됨을 의미합니다.

6. **정규화** 옵션을 선택하여 출력 값을 확률로 변환합니다. 

    변환된 값을 정수로 표시하는 대신 출력 및 기능 데이터 세트의 값은 다음과 같이 변환됩니다.

    + 데이터 세트의 값은 `P(topic|document)`인 확률로 표시됩니다.

    + 기능 토픽 매트릭스의 값은 `P(word|topic)`인 확률로 표시됩니다.

    > [!NOTE] 
    > Azure Machine Learning 디자이너에서 scikit-learn 라이브러리는 버전 0.19의 정규화되지 않은 *doc_topic_distr* 출력을 더 이상 지원하지 않습니다. 이 구성 요소에서 **Normalize** 매개 변수는 *기능 토픽 행렬* 출력에만 적용할 수 있습니다. ‘변환된 데이터 세트’ 출력은 항상 정규화됩니다.

7. 다음과 같은 고급 매개 변수를 설정하려면 **모든 옵션 표시** 옵션을 선택하고 **TRUE** 로 설정합니다.

    해당 매개 변수는 scikit-learn의 LDA 구현에만 적용됩니다. scikit-learn의 LDA에 대한 몇 가지 좋은 자습서와 공식 [scikit-learn 문서](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)가 있습니다.

    + **Rho 매개 변수**. 토픽 배포 희박도의 이전 확률을 지정합니다. 이 매개 변수는 sklearn의 `topic_word_prior` 매개 변수에 해당합니다. 단어 분포가 플랫으로 예상되는 경우, 즉 모든 단어의 확률이 같다고 가정되는 경우 값 **1** 을 사용합니다. 대부분의 단어가 희박하게 표시될 것으로 예상하는 경우 더 낮은 값으로 설정할 수 있습니다.

    + **알파 매개 변수**. 문서 토픽 가중치 희박도의 이전 확률을 지정합니다. 이 매개 변수는 sklearn의 `doc_topic_prior` 매개 변수에 해당합니다.

    + **예상 문서 수**. 처리될 문서(행) 수의 최적 예상 값을 나타내는 숫자를 입력합니다. 이 매개 변수를 사용하면 구성 요소가 충분한 크기의 해시 테이블을 할당할 수 있습니다. 이 매개 변수는 scikit-learn의 `total_samples` 매개 변수에 해당합니다.

    + **일괄 처리의 크기**. LDA 모델에 전송되는 각 텍스트 일괄 처리에 포함할 행 수를 나타내는 숫자를 입력합니다. 이 매개 변수는 scikit-learn의 `batch_size` 매개 변수에 해당합니다.

    + **학습 업데이트 일정에서 사용되는 반복의 초기 값**. 온라인 학습에서 초기 반복 학습 속도의 가중치를 낮추는 시작 값을 지정합니다. 이 매개 변수는 scikit-learn의 `learning_offset` 매개 변수에 해당합니다.

    + **업데이트 중 반복에 적용되는 능력**. 온라인 업데이트 중 학습 속도를 제어하기 위해 반복 횟수에 적용되는 능력 수준을 나타냅니다. 이 매개 변수는 scikit-learn의 `learning_decay` 매개 변수에 해당합니다.

    + **데이터 패스 수**. 알고리즘이 데이터를 순환하는 최대 횟수를 지정합니다. 이 매개 변수는 scikit-learn의 `max_iter` 매개 변수에 해당합니다.

8. 텍스트를 분류하기 전에 초기 패스에서 N-Gram 목록을 만들려면 **N-Gram 사전 빌드** 또는 **LDA 전에 N-Gram 사전 빌드** 옵션을 선택합니다.

    초기 사전을 미리 만든 경우 나중에 모델을 검토할 때 사전을 사용할 수 있습니다. 결과를 숫자 인덱스가 아닌 텍스트에 매핑할 수 있으면 일반적으로 해석이 더 용이합니다. 그러나 사전을 저장하는 데 시간이 오래 걸리고 추가 스토리지가 사용됩니다.

9. **N-Gram 사전의 최대 크기** 에 N-Gram 사전에서 만들 수 있는 행의 총수를 입력합니다.

    이 옵션은 사전 크기를 제어하는 데 유용합니다. 그러나 입력의 N-Gram 수가 이 크기를 초과할 경우 충돌이 발생할 수 있습니다.

10. 파이프라인을 제출합니다. LDA 구성 요소는 Bayes 정리를 사용하여 개별 단어와 연결될 수 있는 항목을 결정합니다. 단어는 토픽 또는 그룹과 단독으로 연결되지 않습니다. 대신, 각 N-Gram에는 검색된 클래스와 연결될 것으로 학습된 확률이 있습니다.

## <a name="results"></a>결과

구성 요소에는 두 개의 출력이 있습니다.

+ **변환된 데이터 세트**: 이 출력에는 입력 텍스트, 지정된 개수의 검색된 범주, 범주별 각 텍스트 예제의 점수가 포함됩니다.

+ **기능 토픽 매트릭스**: 맨 왼쪽 열에는 추출된 텍스트 기능이 포함됩니다. 각 범주 열에는 범주별 해당 기능의 점수가 포함됩니다.


### <a name="lda-transformation"></a>LDA 변환

이 구성 요소는 데이터 세트에 *LDA를* 적용하는 LDA 변환도 출력합니다.

변환을 저장하고 다른 데이터 세트에 재사용할 수 있습니다. 이 기법은 큰 코퍼스에서 학습했으며 계수 또는 범주를 재사용하려는 경우에 유용할 수 있습니다.

이 변환을 다시 사용하려면 Latent Dirichlet 할당 구성 요소의 오른쪽 패널에서 **데이터 세트 등록** 아이콘을 선택하여 구성 요소 목록의 데이터 세트 범주 아래에 구성 요소를 **유지합니다.** 그런 다음 이 구성 요소를 [변환 적용](apply-transformation.md) 구성 요소에 연결하여 이 변환을 다시 사용할 수 있습니다.

### <a name="refining-an-lda-model-or-results"></a>LDA 모델 또는 결과 구체화

일반적으로 모든 요구 사항을 충족하는 단일 LDA 모델을 만들 수는 없습니다. 한 작업용으로 설계된 모델도 정확도 향상을 위해 많은 반복이 필요할 수 있습니다. 다음 방법을 모두 시도하여 모델을 개선하는 것이 좋습니다.

+ 모델 매개 변수 변경
+ 시각화를 사용하여 결과 이해
+ 실무 전문가의 피드백을 받아 생성된 토픽이 유용한지 확인

질적 측정값은 결과를 평가하는 데에도 유용할 수 있습니다. 토픽 모델링 결과를 평가하려면 다음을 고려합니다.

+ 정확도. 유사한 항목이 정말 유사한가요?
+ 다양성. 비즈니스 문제에 필요한 경우 모델이 유사 항목을 구분할 수 있나요?
+ 확장성 광범위한 텍스트 범주에서 작동하나요, 아니면 좁은 대상 도메인에서만 작동하나요?

자연어 처리를 통해 텍스트를 정리, 요약, 간소화 또는 분류하여 LDA를 기반으로 하는 모델의 정확도를 향상할 수 있는 경우도 많습니다. 예를 들어 Azure Machine Learning에서 모두 지원되는 다음 기법은 분류 정확도를 높일 수 있습니다.

+ 중지 단어 제거

+ 대/소문자 정규화

+ 표제어 분석 또는 형태소 분석

+ 명명된 엔터티 인식

자세한 내용은 [텍스트 전처리](preprocess-text.md)를 참조하세요.

디자이너에서 텍스트 처리에 R 또는 Python 라이브러리를 사용할 수도 있습니다. [R 스크립트 실행](execute-r-script.md) 및 [Python 스크립트 실행](execute-python-script.md)을 참조하세요.



## <a name="technical-notes"></a>기술 정보

이 섹션에는 구현 정보, 팁, 질문과 대답이 포함되어 있습니다.

### <a name="implementation-details"></a>구현 세부 정보

기본적으로 변환된 데이터 세트 및 기능-토픽 매트릭스의 출력 분포는 확률로 정규화됩니다.

+ 변환된 데이터 세트는 문서가 지정된 경우 토픽의 조건부 확률로 정규화됩니다. 이 경우 각 행의 합계는 1과 같습니다.

+ 기능-토픽 매트릭스는 토픽이 지정된 경우 단어의 조건부 확률로 정규화됩니다. 이 경우 각 열의 합계는 1과 같습니다.

> [!TIP]
> 경우에 따라 구성 요소가 빈 항목을 반환할 수도 있습니다. 대체로 원인은 알고리즘의 의사 난수 초기화입니다. 이 문제가 발생할 경우 관련 매개 변수를 변경해 볼 수 있습니다. 예를 들어 N-Gram 사전의 최대 크기 또는 기능 해시에 사용할 비트 수를 변경합니다.

### <a name="lda-and-topic-modeling"></a>LDA 및 토픽 모델링

잠재적 Dirichlet 할당은 기본적으로 미분류 텍스트에서 범주 학습을 의미하는 ‘콘텐츠 기반 토픽 모델링’에 자주 사용됩니다. 콘텐츠 기반 토픽 모델링에서 토픽은 단어 분포입니다.

예를 들어 여러 제품을 포함하는 고객 리뷰 코퍼스를 제공했다고 가정합니다. 시간이 지남에 따라 고객이 제출한 리뷰 텍스트에는 많은 용어가 포함되고, 일부 용어는 여러 토픽에서 사용되었습니다.

LDA 프로세스에서 식별된 ‘토픽’은 개별 제품에 대한 리뷰를 나타내거나 제품 리뷰 그룹을 나타낼 수 있습니다. LDA에서 토픽 자체는 시간에 따른 단어 집합의 확률 분포일 뿐입니다.

용어가 한 제품에만 국한되는 경우는 거의 없습니다. 다른 제품을 참조하거나 모든 제품에 적용되는 일반 용어(“멋진”, “놀라운”)일 수 있습니다. 다른 용어는 의미 없는 단어일 수 있습니다. 그러나 LDA 방법은 모든 단어를 캡처하거나 공동 발생 가능성을 제외하고 단어 간 관계를 파악하려고 시도하지 않습니다. 대상 도메인에서 사용되는 단어의 그룹화만 가능합니다.

용어 인덱스가 컴퓨팅된 후 거리 기반 유사성 측정값이 개별 텍스트 행을 비교하여 두 텍스트 부분이 유사한지 확인합니다. 예를 들어 제품에 강력한 상관 관계를 가진 여러 이름이 있는 것을 발견할 수 있습니다. 또는 일반적으로 특정 제품에 강력한 부정적 용어가 연결되는 것을 발견할 수도 있습니다. 유사성 측정값을 사용하여 관련 용어를 식별하고 권장 구성을 만들 수 있습니다.

###  <a name="component-parameters"></a>구성 요소 매개 변수

|Name|Type|범위|Optional|기본값|Description|  
|----------|----------|-----------|--------------|-------------|-----------------|  
|대상 열|열 선택||필수|StringFeature|대상 열 이름 또는 인덱스입니다.|  
|모델링할 토픽 수|정수|[1;1000]|필수|5|N개 토픽의 문서 분포를 모델링합니다.|  
|N그램|정수|[1;10]|필수|2|해시 중에 생성되는 N-Gram의 순서입니다.|  
|Normalize|부울|True 또는 False|필수|true|출력을 확률로 정규화합니다.  변환된 데이터 세트는 P(topic&#124;document)가 되고 기능 토픽 매트릭스는 P(word&#124;topic)이 됩니다.|  
|모든 옵션 표시|부울|True 또는 False|필수|False|scikit-learn 온라인 LDA와 관련된 추가 매개 변수를 제공합니다.|  
|Rho 매개 변수|Float|[0.00001;1.0]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|0.01|토픽 단어 이전 분포입니다.|  
|알파 매개 변수|Float|[0.00001;1.0]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|0.01|문서 토픽 이전 분포입니다.|  
|예상 문서 수|정수|[1;int.MaxValue]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|1000|예상 문서 수입니다. `total_samples` 매개 변수에 해당합니다.|  
|일괄 처리의 크기|정수|[1;1024]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|32|일괄 처리의 크기입니다.|  
|학습 속도 업데이트 일정에서 사용되는 반복의 초기 값|정수|[0;int.MaxValue]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|0|초기 반복 학습 속도의 가중치를 낮추는 초기 값입니다. `learning_offset` 매개 변수에 해당합니다.|  
|업데이트 중 반복에 적용되는 능력|Float|[0.0;1.0]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|0.5|학습 속도를 제어하기 위해 반복 횟수에 적용되는 능력입니다. `learning_decay` 매개 변수에 해당합니다. |  
|학습 반복 횟수|정수|[1;1024]|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|25|학습 반복 횟수입니다.|  
|N-Gram 사전 빌드|부울|True 또는 False|**모든 옵션 표시** 확인란을 선택하지 ‘않은’ 경우에 적용됩니다.|True|LDA를 컴퓨팅하기 전에 N-Gram 사전을 빌드합니다. 모델 검사 및 해석에 유용합니다.|  
|N-Gram 사전의 최대 크기|정수|[1;int.MaxValue]|**N-Gram 사전 빌드** 옵션이 **True** 인 경우에 적용됩니다.|20000|N-Gram 사전의 최대 크기입니다. 입력의 토큰 수가 이 크기를 초과할 경우 충돌이 발생할 수 있습니다.|  
|기능 해시에 사용할 비트 수입니다.|정수|[1;31]|**모든 옵션 표시** 확인란을 선택하지 ‘않았으며’ **N-Gram 사전 빌드** 가 **False** 인 경우에 적용됩니다.|12|기능 해시에 사용할 비트 수입니다.| 
|LDA 전에 N-Gram 사전 빌드|부울|True 또는 False|**모든 옵션 표시** 확인란을 선택한 경우에 적용됩니다.|True|LDA 전에 N-Gram 사전을 빌드합니다. 모델 검사 및 해석에 유용합니다.|  
|사전에 있는 최대 N-Gram 수|정수|[1;int.MaxValue]|**모든 옵션 표시** 확인란을 선택했으며 **N-Gram 사전 빌드** 옵션이 **True** 인 경우에 적용됩니다.|20000|사전의 최대 크기입니다. 입력의 토큰 수가 이 크기를 초과할 경우 충돌이 발생할 수 있습니다.|  
|해시 비트 수|정수|[1;31]|**모든 옵션 표시** 확인란을 선택했으며 **N-Gram 사전 빌드** 옵션이 **False** 인 경우에 적용됩니다.|12|기능 해시 중에 사용할 비트 수입니다.|   


## <a name="next-steps"></a>다음 단계

Azure Machine Learning 하는 데 [사용할 수 있는 구성 요소 집합](component-reference.md) 을 참조 하세요. 

구성 요소와 관련 된 오류 목록은 [디자이너의 예외 및 오류 코드](designer-error-codes.md)를 참조 하세요.
