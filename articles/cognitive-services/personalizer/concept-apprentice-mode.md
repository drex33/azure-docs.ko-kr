---
title: 실습생 모드 - Personalizer
description: 코드 변경 없이 실습생 모드를 사용하여 모델에 대한 자신감을 쌓는 방법을 알아봅니다.
author: jeffmend
ms.author: jeffme
ms.manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 05/01/2020
ms.openlocfilehash: aabda26cd2a1dea7205c8432f11af35e3f20b0e1
ms.sourcegitcommit: 16e25fb3a5fa8fc054e16f30dc925a7276f2a4cb
ms.translationtype: HT
ms.contentlocale: ko-KR
ms.lasthandoff: 08/25/2021
ms.locfileid: "122830876"
---
# <a name="use-apprentice-mode-to-train-personalizer-without-affecting-your-existing-application"></a>기존 애플리케이션에 미치는 영향 없이 실습생 모드로 Personalizer 학습

**실제** 보충 학습에서의 특성 때문에, Personalizer 모델은 프로덕션 환경에서만 교육할 수 있습니다. 모델의 충분한 학습에는 시간이 소요되므로 새 사용 사례를 배포할 때는 Personalizer 모델이 효율적으로 수행되지 않습니다.  **실습생 모드** 를 통해 이러한 상황을 완화하고 개발자의 코드 변경 없이도 모델에 대한 자신감을 쌓을 수 있습니다.

[!INCLUDE [Important Blue Box - Apprentice mode pricing tier](./includes/important-apprentice-mode.md)]

## <a name="what-is-apprentice-mode"></a>실습생 모드란?

실습생이 전문가의 기술을 배우면서 경험을 통해 점차 나아지듯이, 기존 애플리케이션 로직에서 얻은 결과를 Personalizer가 관찰하여 학습할 수 있게 하는 _동작_ 입니다.

Personalizer는 애플리케이션과 동일한 출력을 모방하여 학습합니다. 이벤트 흐름이 많을수록 Personalizer가 기존 로직이나 결과에 미치는 영향 없이 기존 애플리케이션을 _따라잡을_ 수 있습니다. Azure Portal 및 API에서 사용할 수 있는 메트릭은 모델의 학습 성과를 이해하는 데 도움이 됩니다.

Personalizer가 학습을 통해 일정 수준의 이해를 확보한 후에는 개발자가 동작을 실습생 모드에서 온라인 모드로 변경할 수 있습니다. 이 시점이 되면 Personalizer가 Rank API의 작업에 영향을 미치기 시작합니다.

## <a name="purpose-of-apprentice-mode"></a>실습생 모드의 용도

실습생 모드는 Personalizer 서비스 및 기계 학습 기능을 신뢰하고, 서비스가 온라인 트래픽에 위험을 초래하지 않고 학습 가능한 정보를 보내고 있음을 재확인할 수 있게 합니다.

실습생 모드를 사용하는 두 가지 주요 이유는 다음과 같습니다.

* **콜드 부팅** 완화: 실습생 모드는 최상의 조치를 반환하지 않고 약 60-80%의 만족스러운 수준의 효과를 달성하지 못하는 경우 "새로운" 모델의 학습 시간을 관리하고 평가하는 데 도움이 됩니다.
* **작업 및 컨텍스트 기능 유효성 검사**: 작업 및 컨텍스트에서 보낸 기능은 부적절하거나 부정확할 수 있습니다. Personalizer가 이상적인 효과 수준을 획득하기에는 너무 적거나, 과다하거나, 부정확하거나, 너무 특정할 수 있습니다. [기능 평가](concept-feature-evaluation.md)를 사용하여 기능 관련 문제를 찾아 해결할 수 있습니다.

## <a name="when-should-you-use-apprentice-mode"></a>실습생 모드를 사용해야 하는 경우

Personalizer의 영향을 받지 않는 사용자의 환경은 그대로 두고 다음 시나리오를 통해 Personalizer를 학습시켜 효과를 증대하기 위해 실습생 모드를 사용할 수 있습니다.

* 새 사용 사례에서 Personalizer을 구현합니다.
* 컨텍스트 또는 작업에서 보내는 기능을 크게 변경했습니다.
* 보상을 계산하는 방법과 방법이 크게 변경되었습니다.

실습생 모드는 Personalizer가 보상 점수에 미치는 영향을 측정하는 효과적인 방법이 아닙니다. 각 Rank 호출에 대해 가능한 최적의 작업을 선택하는 데 Personalizer가 어떻게 효과가 있는지 측정하기 위해 [오프라인 평가](concepts-offline-evaluation.md)를 사용합니다.

## <a name="who-should-use-apprentice-mode"></a>실습생 모드를 사용해야 하는 사람

실습생 모드는 개발자, 데이터 과학자 및 비즈니스 의사 결정자에게 유용합니다.

* **개발자** 는 실습생 모드를 통해 Rank 및 Reward API가 애플리케이션에서 정확하게 사용되고 있고, 애플리케이션에서 Personalizer에 보내는 기능이 버그나 무관한 기능(예: 타임스탬프, 사용자 ID 요소)이 포함되지 않음을 확인할 수 있습니다.

* **데이터 과학자** 는 실습생 모드를 사용하여 기능이 Personalizer 모델 교육에 효과적이며, 보상 대기 시간이 너무 길거나 짧지 않은지 유효성을 검사할 수 있습니다.

* **비즈니스 의사 결정자** 는 실습생 모드를 통해, 기존 비즈니스 논리와 비교하여 결과(즉, 보상)를 향상시키는 데 있어 Personalizer의 가능성을 평가할 수 있습니다. 이렇게 하면 실제 매출과 사용자 만족을 좌우하는 사용자 환경에 영향이 있는 합리적 의사 결정을 내릴 수 있습니다.

## <a name="comparing-behaviors---apprentice-mode-and-online-mode"></a>동작 비교 - 실습생 모드 및 온라인 모드

다음과 같은 방법으로 실습생 모드와 온라인 모드의 차이를 학습합니다.

|영역|실습생 모드|온라인 모드|
|--|--|--|
|사용자 경험에 미치는 영향|기존 사용자 동작을 사용하여, Personalizer가 **기본 작업** 이 어땠을 것이며 받게 되는 보상은 무엇인지를 관찰만(영향 없음) 하도록 하여 교육할 수 있습니다. 즉, 사용자의 경험과 그에 따른 비즈니스 결과에는 영향이 없습니다.|사용자 동작에 영향을 주는, Rank 호출이 반환한 주요 작업을 표시합니다.|
|학습 속도|Personalizer는 온라인 모드에서 학습할 때보다 실습생 모드에서의 학습이 더 느립니다. 실습생 모드는 **기본 작업** 에서 얻은 보상을 관찰하는 것으로만 학습할 수 있는데, 탐색을 수행할 수 없기 때문에 학습 속도가 제한됩니다.|현재 모델을 활용하고 새 추세를 살펴볼 수 있으므로 더 빠르게 학습합니다.|
|학습 효과 "천장"|Personalizer는 근접하지만 거의 일치하지 않으며 기본 비즈니스 논리의 성능(각 순위 호출의 **기본 작업** 에 의해 달성된 보상 합계)을 초과할 수 없습니다. 이 근사치 한도는 탐색만큼 줄어듭니다. 예를 들어 20% 탐색에서는 실습생 모드 성능이 80%를 초과할 가능성이 매우 낮고, 60%면 마치고 온라인 모드로 이동할 수 있는 합당한 목표입니다.|Personalizer는 애플리케이션 기준을 초과해야 하며, 지속적 모델 향상을 위해 시간 경과에 따라 중단 지점에서 오프라인 평가 및 기능 평가를 수행해야 합니다. |
|rewardActionId에 대한 Rank API 값|_rewardActionId_ 는 항상 Rank 요청에서 전송하는 첫 번째 작업이므로 사용자 환경에는 영향이 없습니다. 즉, 실습생 모드인 동안은 Rank API가 애플리케이션에서 보이지 않습니다. 애플리케이션의 Reward API가 서로 다른 모드 간에 Reward API를 사용하는 방법을 바꿔서는 안 됩니다.|사용자 환경은 Personalizer가 애플리케이션에 대해 선택하는 _rewardActionId_ 가 변경합니다. |
|평가|Personalizer는 기본 비즈니스 논리가 가져오는 보상 합계와 해당 시점의 온라인 모드에서 Personalizer가 가져올 수 있는 보상 합계를 비교합니다. 리소스에 대한 비교는 Azure Portal에서 사용할 수 있습니다.|[오프라인 평가](concepts-offline-evaluation.md)를 실행하여 Personalizer의 효과를 평가합니다. 이렇게 애플리케이션 기준의 잠재적 보상과 대비하여 Personalizer가 달성한 총 보상을 비교할 수 있습니다.|

실습생 모드의 효과에 대한 참고:

* 실습생 모드에서의 Personalizer 효과는 애플리케이션 기준에 100% 가까이 가는 경우가 거의 없고 이를 초과할 수도 없습니다.
* 모범 사례는 100% 달성을 시도하지 않는 것입니다. 사용 사례에 따라 60% – 80%범위를 목표로 해야 합니다.

## <a name="limitations-of-apprentice-mode"></a>실습생 모드의 제한 사항
실습생 모드는 순위 호출에 사용된 컨텍스트 및 작업과 Reward 호출의 피드백에 있는 기능을 사용하여 기준 항목을 선택하는 기존 알고리즘을 모방하여 Personalizer 모델을 학습시키려고 시도합니다. 다음 요소는 Personalizer 실습생이 일치하는 보상을 충분히 학습하는지 여부 또는 학습하는 시기에 영향을 미칩니다.

### <a name="scenarios-where-apprentice-mode-may-not-be-appropriate"></a>실습생 모드가 적절하지 않을 수 있는 시나리오:

#### <a name="editorially-chosen-content"></a>편집자로서 선택한 콘텐츠:
뉴스 또는 엔터테인먼트와 같은 일부 시나리오에서는 편집 팀에서 기준 항목을 수동으로 할당할 수 있습니다. 즉, 인간은 광범위한 세계에 대한 지식을 사용하고, 매력적인 콘텐츠가 무엇인지 이해하고, 풀에서 특정 문서 또는 미디어를 선택하고, "선호" 또는 "대표" 기사로 플래그를 지정합니다. 이러한 편집기는 알고리즘이 아니며, 편집기에서 고려하는 요소는 미묘한 차이가 있고 컨텍스트 및 작업의 기능으로 포함되지 않을 수 있으므로 실습생 모드는 다음 기준 작업을 예측할 가능성이 낮습니다. 이러한 상황에서 다음을 수행할 수 있습니다.

* 온라인 모드에서 Personalizer 테스트: 실습생 모드에서 기준을 예측하지 않는다고 해서 Personalizer가 더 좋거나 더 나은 결과를 얻을 수 없다는 의미는 아닙니다. 인프라가 있는 경우 일정 기간 동안 또는 A/B 테스트에 Personalizer를 온라인 모드로 전환한 다음, 오프라인 평가를 실행하여 차이를 평가하는 것이 좋습니다.
* 편집 고려 사항 및 권장 사항을 기능으로 추가: 편집기에게 선택 사항에 영향을 미치는 요소를 요청하고, 컨텍스트 및 작업에서 기능으로 추가할 수 있는지 확인합니다. 예를 들어 미디어 회사의 편집기는 특정 유명인의 뉴스에서 콘텐츠를 강조 표시할 수 있습니다. 이 지식은 컨텍스트 기능으로 추가될 수 있습니다.

### <a name="factors-that-will-improve-and-accelerate-apprentice-mode"></a>실습생 모드를 개선하고 가속화하는 요소
실습생 모드가 0을 초과하여 일치하는 보상을 학습하고 획득하지만 속도가 느려지는 것 같은 경우(60%에 도달하지 않음, 2주 이내에 80%의 일치하는 보상) 과제에 데이터가 너무 적을 수 있습니다. 다음 단계를 수행하면 학습을 가속화할 수 있습니다. 

1. 시간이 지남에 따라 긍정적인 보상으로 더 많은 이벤트 추가: 실습생 모드는 애플리케이션이 하루에 100개가 넘는 긍정 보상을 받는 사용 사례에서 더 잘 수행됩니다. 예를 들어, 클릭 한 번으로 만족하는 웹사이트의 클릭 수가 2%일 경우 주목할 만한 학습이 이루어지려면 하루 최소 5,000회 이상 방문해야 합니다. 
2. 더 간단하고 더 자주 발생하는 보상 점수를 사용해보세요. 예를 들어 "사용자가 문서 읽기를 완료했나요?"에서 "사용자가 문서 읽기를 시작했나요"로 진행하는 경우를 예로 들어 보겠습니다.
3. 차별화 기능 추가: 순위 호출에서 작업 및 해당 기능을 시각적으로 검사할 수 있습니다. 기준 작업에 다른 작업과 차별화된 기능이 있나요? 거의 동일하게 보이면 덜 비슷하게 만드는 기능을 더 추가합니다.
4. 이벤트당 작업 감소: Personalizer는 탐색 % 설정을 사용하여 기본 설정 및 추세를 검색합니다. 순위 호출에 더 많은 작업이 있는 경우 탐색을 위해 작업이 선택될 가능성이 낮아집니다. 각 순위 호출에서 전송된 작업의 수를 더 작은 숫자로, 10개 미만으로 줄입니다. 이는 보상과 일치하는 적절한 데이터가 있는 실습생 모드를 보여 주는 임시 조정일 수 있습니다.


## <a name="using-apprentice-mode-to-train-with-historical-data"></a>실습생 모드에서 과거 데이터로 학습

양이 많은 과거 데이터로 Personalizer를 교육하려는 경우 실습생 모드를 사용하여 Personalizer에서 데이터를 재생할 수 있습니다.

Personalizer를 실습생 모드로 설정하고 과거 데이터의 작업과 컨텍스트 기능으로 Rank를 호출하는 스크립트를 만듭니다. 이 데이터의 레코드 계산을 기준으로 Reward API를 호출합니다. 어느 정도의 결과를 보려면 약 50,000개의 과거 이벤트가 필요하지만 결과에 대한 신뢰를 높이려면 500,000개가 권장됩니다.

과거 데이터로부터 학습할 때는 컨텍스트 및 작업에 대한 기능, Rank 요청에 사용되는 JSON의 레이아웃, 이 학습 데이터 세트의 보상 계산에서 보내는 데이터가, 기존 애플리케이션에서 사용할 수 있는 데이터(보상의 기능 및 계산)와 일치해야 합니다.

오프라인 및 사후 데이터는 불완전하고 더 지저분한 경향이 있고 형식이 다릅니다. 과거 데이터를 통한 학습은 가능하나 그에 따른 결과는 결정적이지 못하고, Personalizer가 얼마나 잘 학습할 것인지 예측하기 위한 지표는 되지 못할 수 있습니다. 특히 과거 데이터와 기존 애플리케이션 사이에 기능 차이가 있는 경우가 그렇습니다.

일반적으로 과거 데이터를 통한 학습과 비교할 때 Personalizer에서는 실습생 모드로 변경하고 기존 애플리케이션으로부터 학습하는 것이 더 적은 노력, 데이터 엔지니어링 및 정리 작업으로 효과적인 모델을 구현할 수 있는 더 효과적인 방법이 됩니다.

## <a name="using-apprentice-mode-versus-ab-tests"></a>실습생 모드와 A/B 테스트 사용 비교

유효성을 검사했고 온라인 모드에서 학습 중인 경우에만 Personalizer 처리의 A/B 테스트가 유용합니다. 실습생 모드에서는 **기본 동작** 만 사용하므로 모든 사용자가 제어 환경을 효과적으로 볼 수 있습니다.

Personalizer가 _처리_ 에 한정된 경우라도, Personalizer 학습에 적합한 데이터인지에 대한 유효성을 검사할 때는 같은 어려움이 있습니다. 대신 100% 트래픽과, 제어(영향을 받지 않음) 환경을 가져오는 모든 사용자로 실습생 모드를 사용할 수 있습니다.

Personalizer를 사용하고 온라인으로 학습하는 사용 사례가 갖추어지면, 보상에 사용한 신호보다 더 복잡할 수 있는 결과의 과학적 비교와 제어되는 코호트를 A/B 실험을 통해 수행할 수 있습니다. A/B 테스트에서 답할 수 있는 예제 질문은 `In a retail website, Personalizer optimizes a layout and gets more users to _check out_ earlier, but does this reduce total revenue per transaction?`입니다.

## <a name="next-steps"></a>다음 단계

* [활성 및 비활성 이벤트](concept-active-inactive-events.md) 알아보기
