### YamlMime:FAQ
metadata:
  title: Azure Video Analyzer FAQ - Azure
  description: 이 문서에서는 Azure Video Analyzer에 대해 자주 묻는 질문에 답변합니다.
  ms.service: azure-video-analyzer
  ms.topic: conceptual
  ms.date: 06/01/2021
  ms.openlocfilehash: d7cd8c04c55f27426486ed0853f2450946e5c07b
  ms.sourcegitcommit: f6e2ea5571e35b9ed3a79a22485eba4d20ae36cc
  ms.translationtype: MT
  ms.contentlocale: ko-KR
  ms.lasthandoff: 09/24/2021
  ms.locfileid: "130555844"
title: Azure Video Analyzer FAQ
summary: 이 문서에서는 Azure Video Analyzer에 대해 일반적으로 묻는 질문에 답변합니다.
sections:
- name: 무시됨
  questions:
  - question: 일반
    answer: "**파이프라인 토폴로지 정의에서 사용할 수 있는 시스템 변수는 무엇인가요?**\n\n| 변수   |  설명  | \n| --- | --- | \n| System.Runtime.DateTime | 일반적으로 다음 형식의 날짜와 시간으로 표현되는 UTC 시간의 한 순간을 나타냅니다.<br>*yyyyMMddTHHmmssZ* | \n| System.Runtime.PreciseDateTime | UTC(협정 세계시) 날짜/시간 인스턴스를 다음과 같은 밀리초 단위의 ISO8601 파일 호환 형식으로 나타냅니다.<br>*yyyyMMddTHHmmss.fffZ* | \n| System.TopologyName    | 파이프라인 토폴로지의 이름을 나타냅니다. | \n| System.PipelineName | 라이브 파이프라인의 이름을 나타냅니다. | \n\n> [!Note] \n> System.Runtime.DateTime 및 System.Runtime.PreciseDateTime은 비디오 싱크 노드에서 Azure Video Analyzer 비디오 리소스 이름의 일부로 사용할 수 없습니다. 이러한 변수는 파일 이름 지정을 위해 FileSink 노드에서 사용할 수 있습니다.\n\n**Video Analyzer에 대한 개인정보처리방침은 무엇인가요?**\n\nVideo Analyzer에는 [Microsoft 개인정보처리방침](https://privacy.microsoft.com/privacystatement)이 적용됩니다. 개인정보처리방침은 Microsoft에서 처리하는 개인 데이터, Microsoft에서 개인 데이터를 처리하는 방법 및 Microsoft에서 개인 데이터를 처리하는 목적을 설명합니다. 개인 정보에 대해 자세히 알아보려면 [Microsoft 보안 센터](https://www.microsoft.com/trustcenter)를 방문하세요.\n"
  - question: 구성 및 배포
    answer: "**Edge 모듈을 Windows 10 디바이스에 배포할 수 있나요?**\n\n예. 자세한 내용은 [Windows 10의 Linux 컨테이너](/virtualization/windowscontainers/deploy-containers/linux-containers)를 참조하세요.\n\n**가상 머신을 시뮬레이션된 에지 디바이스로 사용하고 있습니다. 배포 후 VM 구성의 크기를 변경할 수 있나요?**\n\n예, Azure Video Analyzer와 같은 에지 모듈이 배포된 후 [VM을 다른 크기로 확장하거나 축소할](../../virtual-machines/resize-vm.md) 수 있습니다. 가상 머신에서 IoT Edge 모듈을 다시 시작해야 할 수 있습니다. Linux VM의 경우 명령을 `sudo systemctl restart iotedge` 사용합니다. 라이브 파이프라인이 실행 중인 경우 다시 시작이 완료된 후 다시 시작해야 합니다. 다시 시작하는 동안 파이프라인의 출력(예: 유추 이벤트, 비디오 녹화)에 인터럽션이 표시됩니다.\n\n**IoT Edge 디바이스의 RTSP 카메라 시뮬레이터 모듈에 샘플 비디오 파일을 추가하려면 어떻게 해야 합니까?**\n\n기존 IoT Edge 디바이스 폴더에 새 비디오 파일을 추가하려면 \n\n- Azure Storage [계정의](../../storage/blobs/storage-quickstart-blobs-portal.md#upload-a-block-blob) 새 Blob 컨테이너 또는 기존 Blob 컨테이너에 비디오 파일 업로드\n- 업로드되면 Blob 개요 -> 속성으로 이동하여 업로드된 파일의 URL을 형식으로 가져옵니다. `https://<storage-account-name>.blob.core.windows.net/<container-name>/<videofile.mkv>`\n- 에지 디바이스 터미널에 로그인하고 명령을 `sudo curl https://<storage-account-name>.blob.core.windows.net/<container-name>/<videofile.mkv> --output /home/localedgeuser/samples/input/<videofile.mkv>` 실행합니다. 선택한 다른 출력 경로를 선택할 수 있습니다.\n\n테스트에 사용할 수 있는 샘플 미디어 파일은 미디어 데이터 세트 에서 찾을 수 [있습니다.](https://github.com/Azure/video-analyzer/tree/main/media)\n"
  - question: IP 카메라 및 RTSP 설정에서 캡처
    answer: "**비디오 스트림으로 보내려면 디바이스에서 특별한 SDK를 사용해야 하나요?**\n\n아니요, Video Analyzer는 대부분의 IP 카메라에서 지원되는 RTSP(실시간 스트리밍 프로토콜)를 비디오 스트리밍에 사용하여 미디어를 캡처할 수 있습니다.\n\n**RTSP 이외의 프로토콜을 사용하여 Video Analyzer에 미디어를 푸시할 수 있나요?**\n\n아니요, Video Analyzer는 IP 카메라에서 비디오를 캡처하는 데 RTSP만 지원합니다. TCP/HTTP를 통한 RTSP 스트리밍을 지원하는 모든 카메라가 작동합니다. \n\n**라이브 파이프라인에서 RTSP 원본 URL을 다시 설정하거나 업데이트할 수 있나요?**\n\n예, 라이브 파이프라인이 *비활성* 상태인 경우에 가능합니다.  \n\n**테스트 및 개발 중에 RTSP 시뮬레이터를 사용할 수 있나요?**\n\n예, [RTSP 시뮬레이터](https://github.com/Azure/video-analyzer/tree/main/edge-modules/sources/rtspsim-live555) 에지 모듈은 학습 프로세스를 지원하는 빠른 시작 및 자습서에서 사용할 수 있습니다. 이 모듈은 최선의 노력으로 제공되며 항상 사용할 수 있는 것은 아닙니다. 시뮬레이터를 몇 시간 이상 사용하지 *않는* 것이 좋습니다. 프로덕션 배포를 계획하기 전에 실제 RTSP 원본을 사용하여 테스트하는 데 투자해야 합니다.\n"
  - question: AI 모델 디자인
    answer: "**여러 AI 모델이 Docker 컨테이너에 래핑되어 있습니다. Azure Video Analyzer에서 사용하려면 어떻게 해야 하나요?** \n\n솔루션은 Azure Video Analyzer와 통신하기 위해 추론 서버에서 사용하는 통신 프로토콜에 따라 달라집니다. 다음 섹션에서는 각 프로토콜의 작동 방식에 대해 설명합니다.\n\n*HTTP 프로토콜 사용*:\n\n* 단일 컨테이너(*avaextension* 으로 명명된 모듈):  \n\n   추론 서버에서 단일 포트를 사용할 수 있지만, 서로 다른 엔드포인트를 여러 AI 모델에 사용할 수 있습니다. 예를 들어 Python 샘플의 경우 다음과 같이 모델별로 서로 다른 `routes`를 사용할 수 있습니다. \n\n   ```\n   @app.route('/score/face_detection', methods=['POST']) \n   … \n   Your code specific to face detection model\n\n   @app.route('/score/vehicle_detection', methods=['POST']) \n   … \n   Your code specific to vehicle detection model \n   … \n   ```\n\n   그런 다음, Video Analyzer 배포에서 라이브 파이프라인을 활성화할 때 각각에 대한 추론 서버 URL을 다음과 같이 설정합니다. \n\n   첫 번째 라이브 파이프라인: 추론 서버 URL=`http://avaextension:44000/score/face_detection`<br/>\n   두 번째 라이브 파이프라인: 추론 서버 URL=`http://avaextension:44000/score/vehicle_detection`  \n   \n    > [!NOTE]\n    > 또는 라이브 파이프라인을 활성화할 때 AI 모델을 다른 포트에 공개하고 호출할 수 있습니다.  \n\n* 여러 컨테이너: \n\n   각 컨테이너는 다른 이름으로 배포됩니다. 빠른 시작과 자습서에서는 *avaextension* 으로 명명된 확장의 배포 방법을 살펴보았습니다. 이제 각각 동일한 HTTP 인터페이스를 사용하는 두 개의 서로 다른 컨테이너를 개발할 수 있습니다. 즉, 동일한 `/score` 엔드포인트가 있습니다. 이러한 두 컨테이너를 다른 이름으로 배포하고, 모두 *서로 다른 포트* 에서 수신 대기하는지 확인합니다. \n\n   예를 들어 `avaextension1`이라는 컨테이너는 `44000` 포트를 수신 대기하고, `avaextension2`라는 두 번째 컨테이너는 `44001` 포트를 수신 대기합니다. \n\n   Video Analyzer 토폴로지에서 다음과 같이 서로 다른 추론 URL을 사용하여 두 개의 라이브 파이프라인을 인스턴스화합니다. \n\n   첫 번째 라이브 파이프라인: 추론 서버 URL = `http://avaextension1:44000/score`    \n   두 번째 라이브 파이프라인: 추론 서버 URL = `http://avaextension2:44001/score`\n   \n*gRPC 프로토콜 사용*: \n\n* gRPC 확장 노드에는 gRPC 계약의 일부로 사용할 수 있는 선택적 문자열인 `extensionConfiguration` 속성이 있습니다. 여러 AI 모델이 하나의 추론 서버에 패키지된 경우 모든 AI 모델에 대한 노드를 공개할 필요가 없습니다. 대신 라이브 파이프라인의 경우 확장 공급자는 `extensionConfiguration` 속성을 사용하여 여러 AI 모델을 선택하는 방법을 정의할 수 있습니다. 실행 중에 Video Analyzer는 이 문자열을 사용자의 추론 서버로 전달하여 원하는 AI 모델을 호출하는 데 사용할 수 있습니다. \n\n**AI 모델을 중심으로 gRPC 서버를 구축하고 있으며, 여러 카메라 또는 라이브 파이프라인에서 이 서버를 사용할 수 있도록 지원하려고 합니다. 어떻게 서버를 구축해야 하나요?** \n\n 먼저 서버에서 둘 이상의 요청을 한 번에 처리하거나 병렬 스레드에서 작업을 수행할 수 있어야 합니다. \n\n예를 들어 다음 [Azure Video Analyzer gRPC 샘플에서](./analyze-live-video-use-your-model-grpc.md)기본 병렬 채널 수가 설정되었습니다. \n\n```\nserver = grpc.server(futures.ThreadPoolExecutor(max_workers=3)) \n```\n\n이전의 gRPC 서버 인스턴스화에서 서버는 카메라당 또는 라이브 파이프라인당 세 개의 채널만 한 번에 열 수 있습니다. 세 개가 넘는 인스턴스는 서버에 연결하지 마세요. 세 개가 넘는 채널을 열려고 하면 기존 채널이 삭제될 때까지 요청이 계속 보류됩니다.  \n\n이전의 gRPC 서버 구현은 Python 샘플에서 사용됩니다. 개발자는 자신의 고유한 서버를 구현하거나 이전 기본 구현을 사용하여 작업자 수를 늘릴 수 있으며, 이를 비디오 피드에 사용할 카메라 수로 설정합니다. \n\n여러 카메라를 설정하고 사용하기 위해 각각 동일하거나 다른 추론 서버(예: 이전 단락에서 언급한 서버)를 가리키는 여러 개의 라이브 파이프라인을 인스턴스화할 수 있습니다. \n\n**추론 의사 결정을 내리기 전에 여러 프레임을 받을 수 있기를 원합니다. 이 기능을 사용하도록 설정하려면 어떻게 해야 하나요?** \n\n현재 [기본 샘플](https://github.com/Azure/video-analyzer/tree/main/edge-modules)은 *상태 비저장* 모드에서 작동합니다. 이전 호출의 상태나 호출자의 ID는 유지하지 않습니다. 즉, 여러 라이브 파이프라인에서 동일한 추론 서버를 호출할 수 있지만 해당 서버에서 호출자 또는 호출자별 상태를 구분할 수 없습니다. \n\n*HTTP 프로토콜 사용*:\n\n상태를 유지하기 위해 각 호출자 또는 라이브 파이프라인은 호출자 고유의 HTTP 쿼리 매개 변수를 사용하여 추론 서버를 호출합니다. 예를 들어 각 라이브 파이프라인의 추론 서버 URL 주소는 다음과 같습니다.  \n\n첫 번째 라이브 파이프라인: `http://avaextension:44000/score?id=1`<br/>\n두 번째 라이브 파이프라인: `http://avaextension:44000/score?id=2`\n\n… \n\n서버 쪽에서는 `id`를 통해 호출자를 식별할 수 있습니다. `id`=1이면 서버는 해당 라이브 파이프라인에 대해 상태를 별도로 유지할 수 있습니다. 그런 다음, 수신된 비디오 프레임을 버퍼에 유지할 수 있습니다. 예를 들어 배열 또는 DateTime 키가 있는 사전을 사용하고, 값은 프레임입니다. 그런 다음, *x* 개의 프레임을 받은 후에 처리(추론)할 서버를 정의할 수 있습니다. \n\n*gRPC 프로토콜 사용*: \n\ngRPC 확장을 사용하면 각 세션이 단일 카메라 피드용이므로 식별자를 제공할 필요가 없습니다. extensionConfiguration 속성을 사용하면 비디오 프레임을 버퍼에 저장하고 *x* 개의 프레임을 받은 후에 처리(추론)할 서버를 정의할 수 있습니다. \n\n**특정 컨테이너의 모든 ProcessMediaStreams에서 동일한 AI 모델을 실행하나요?** \n\n아니요. 라이브 파이프라인에서 최종 사용자의 시작 또는 중지 호출은 세션을 구성하거나 카메라 연결을 끊거나 다시 연결하는 것일 수도 있습니다. 이는 카메라에서 비디오를 스트림하는 경우 하나의 세션을 유지하기 위한 것입니다. \n\n* 처리를 위해 비디오를 별도의 라이브 파이프라인 두 개로 보내는 두 개의 카메라에서 두 개의 세션을 만듭니다. \n* 두 개의 gRPC 확장 노드가 있는 라이브 파이프라인으로 이동하는 하나의 카메라에서 두 개의 세션을 만듭니다. \n\n각 세션은 Video Analyzer와 gRPC 서버 간의 전이중 연결이며, 각 세션마다 다른 모델이 있을 수 있습니다. \n\n> [!NOTE]\n> 카메라의 연결을 끊거나 다시 연결하는 경우 허용 제한을 초과하는 기간 동안 카메라가 오프라인 상태가 되면 Video Analyzer에서 gRPC 서버를 사용하여 세션을 엽니다. 서버에서 이러한 세션의 상태를 추적할 필요가 없습니다. \n\n또한 Video Analyzer는 라이브 파이프라인의 단일 카메라에서 여러 gRPC 확장에 대한 지원을 추가합니다. 이러한 gRPC 확장을 사용하여 AI 처리를 순차적으로, 병렬로 또는 둘 모두를 조합하여 수행할 수 있습니다. \n\n> [!NOTE]\n> 여러 확장을 병렬로 실행하면 하드웨어 리소스에 영향을 줍니다. 컴퓨팅 요구 사항에 맞는 하드웨어를 선택하는 경우 이를 고려해야 합니다. \n\n**동시 ProcessMediaStreams의 최대 수는 얼마인가요?** \n\nVideo Analyzer는 이 수를 제한하지 않습니다.  \n\n**추론 서버에서 CPU 또는 GPU 또는 다른 하드웨어 가속기를 사용해야 하는지 여부를 결정하려면 어떻게 해야 하나요?** \n\n결정은 개발된 AI 모델의 복잡성과 CPU 및 하드웨어 가속기를 사용하는 방법에 따라 달라집니다. AI 모델을 개발할 때 모델에서 사용해야 하는 리소스와 수행해야 하는 작업을 지정할 수 있습니다. \n\n**내 추론 서버에서 생성된 경계 상자를 보려면 어떻게 할까요?** \n\n비디오 리소스의 미디어와 함께 추론 결과를 기록할 수 있습니다. [위젯을](player-widget.md) 사용하여 유추 데이터의 오버레이로 비디오를 재생할 수 있습니다.\n"
  - question: gRPC 호환성
    answer: "**미디어 스트림 설명자에 대한 필수 필드는 어떻게 알 수 있나요?** \n\n값을 제공하지 않는 필드에는 [gRPC에서 지정한 대로 기본값](https://developers.google.com/protocol-buffers/docs/proto3#default)이 지정됩니다.  \n\nVideo Analyzer는 *proto3* 버전의 프로토콜 버퍼 언어를 사용합니다. Video Analyzer 계약에서 사용하는 모든 프로토콜 버퍼 데이터는 [프로토콜 버퍼 파일]()에서 사용할 수 있습니다.<!--add-valid-link.md)--><!--https://github.com/Azure/azree-video-analyzer/tree/master/contracts/grpc-->. \n\n**최신 프로토콜 버퍼 파일을 사용하는지 확인하려면 어떻게 해야 하나요?** \n\n[계약 파일 사이트](https://github.com/Azure/video-analyzer/tree/main/contracts/grpc)에서 최신 프로토콜 버퍼 파일을 가져올 수 있습니다. 계약 파일을 업데이트할 때마다 이 위치에 있습니다. 프로토콜 파일을 즉시 업데이트할 계획이 없으므로 파일의 위쪽에 있는 패키지 이름을 찾아서 버전을 확인합니다. 이 단계는 다음과 같아야 합니다. \n\n```\nmicrosoft.azure.media.live_video_analytics.extensibility.grpc.v1\n```\n\n이러한 파일을 업데이트하면 이름 끝에 있는 \"v-값\"이 증가합니다. \n\n> [!NOTE]\n> Video Analyzer에서 proto3 버전의 언어를 사용하므로 필드는 선택 사항이며 버전은 이전 버전 및 이후 버전과 호환됩니다. \n\n**Video Analyzer에서 사용할 수 있는 gRPC 기능은 무엇인가요? 필수 기능과 선택적 기능은 어떻게 되나요?** \n\n프로토콜 버퍼(Protobuf) 계약이 충족되는 경우 모든 서버 쪽 gRPC 기능을 사용할 수 있습니다.\n"
  - question: 모니터링 및 메트릭
    answer: "**Azure Event Grid를 사용하여 에지에서 파이프라인을 모니터링할 수 있나요?**\n\n예. [Prometheus 메트릭](monitor-log-edge.md#azure-monitor-collection-via-telegraf)을 사용하고 Event Grid에 게시할 수 있습니다. \n\n**Azure Monitor를 사용하여 클라우드 또는 에지에서 파이프라인의 상태, 메트릭 및 성능을 볼 수 있나요?**\n\n예, 이 방법을 지원합니다. 자세한 내용은 [Azure Monitor 메트릭 개요](../../azure-monitor/essentials/data-platform-metrics.md)를 참조하세요.\n\n**Azure Video Analyzer IoT Edge 모듈을 더 쉽게 모니터링할 수 있는 도구가 있나요?**\n\nVisual Studio Code는 Video Analyzer Edge 모듈 엔드포인트를 쉽게 모니터링할 수 있는 Azure IoT Tools 확장을 지원합니다. 이 도구를 사용하여 \"이벤트\"에 대한 IoT 허브 기본 제공 엔드포인트 모니터링을 빠르게 시작하고, 에지 디바이스에서 클라우드로 라우팅되는 추론 메시지를 볼 수 있습니다. \n\n또한 이 확장을 사용하여 Video Analyzer Edge 모듈의 모듈 쌍을 편집하여 파이프라인 설정을 수정할 수 있습니다.\n\n자세한 내용은 [모니터링 및 로깅 문서](monitor-log-edge.md)를 참조하세요.\n"
  - question: 요금 청구 및 가용성
    answer: >
      **Azure Video Analyzer 요금은 어떻게 청구되나요?**


      청구 세부 정보는 [Video Analyzer 가격 책정을 참조하세요.](https://azure.microsoft.com/pricing/details/video-analyzer/)
additionalContent: "\n## <a name=\"next-steps\"></a>다음 단계\n\n[빠른 시작: Azure Video Analyzer 시작](get-started-detect-motion-emit-events.md)"
