---
title: 예제를 통해 Azure Data Factory 가격 책정 이해
description: 이 문서에서는 자세한 예제와 함께 Azure Data Factory 가격 책정 모델을 설명하고 보여줍니다.
author: shirleywangmsft
ms.author: shwang
ms.reviewer: jburchel
ms.service: data-factory
ms.subservice: pricing
ms.topic: conceptual
ms.date: 09/07/2021
ms.openlocfilehash: 45d8b3dea72555470059cb14b3268e4d9b4e9611
ms.sourcegitcommit: 677e8acc9a2e8b842e4aef4472599f9264e989e7
ms.translationtype: MT
ms.contentlocale: ko-KR
ms.lasthandoff: 11/11/2021
ms.locfileid: "132319519"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>예제를 통해 Data Factory 가격 책정 이해

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

이 문서에서는 자세한 예제와 함께 Azure Data Factory 가격 책정 모델을 설명하고 보여줍니다.  더 구체적인 시나리오에 대해서는 [Azure 가격 계산기를](https://azure.microsoft.com/pricing/calculator/) 참조하고 서비스를 사용하기 위해 향후 비용을 예상할 수도 있습니다.

> [!NOTE]
> 아래 예제에 사용된 가격은 가상이며 실제 가격 책정을 의미하지는 않습니다.

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>AWS S3에서 Azure Blob Storage로 매시간 데이터 복사

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트가 있는 복사 작업

2. Azure Storage의 데이터에 대한 출력 데이터 세트

3. 파이프라인을 1시간마다 실행하는 일정 트리거

   :::image type="content" source="media/pricing-concepts/scenario1.png" alt-text="다이어그램은 일정 트리거가 있는 파이프라인을 보여줍니다. 파이프라인에서 복사 작업은 입력 데이터 세트로 흐릅니다. 이 데이터 세트는 A W S3 연결된 서비스로 흐르고 복사 작업은 출력 데이터 세트로도 흐릅니다. 이 데이터 세트는 Azure Storage 연결된 서비스로 흐릅니다.":::

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 2개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 2개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 1개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 2개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 1개) |

**총 시나리오 가격 책정: $0.16811**

- Data Factory 작업 = **$0.0001**
  - 읽기/쓰기 = 10 \* 0.00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = \* 2 0.000005 = $0.00001 [1 모니터링 = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.168**
  - 활동 실행 = 0.001 \* 2 = $0.002 [1 실행 = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>데이터를 복사하고 Azure Databricks를 사용하여 시간별 변환

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하고 Azure Databricks를 사용하여 매시간 일정으로 데이터를 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트 및 Azure Storage의 데이터에 대한 출력 데이터 세트가 있는 하나의 복사 작업
2. 데이터 변환에 대한 하나의 Azure Databricks 작업
3. 파이프라인을 1시간마다 실행하는 하나의 일정 트리거

:::image type="content" source="media/pricing-concepts/scenario2.png" alt-text="다이어그램은 일정 트리거가 있는 파이프라인을 보여줍니다. 파이프라인에서 복사 작업은 Azure Databricks 실행되는 입력 데이터 세트, 출력 데이터 세트 및 DataBricks 작업으로 흐릅니다. 입력 데이터 세트는 A W S3 연결된 서비스로 흐릅니다. 출력 데이터 세트는 Azure Storage 연결된 서비스로 흐릅니다.":::

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 3개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 3개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 2개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 3개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 2개) |
| Databricks 활동 실행 가정: 실행 시간 = 10분 | 10분 외부 파이프라인 활동 실행 |

**총 시나리오 가격 책정: $0.16916**

- Data Factory 작업 = **$0.00012**
  - 읽기/쓰기 = 11 \* 0.00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = \* 3 0.000005 = $0.00001 [1 모니터링 = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.16904**
  - 활동 실행 = 0.001 \* 3 = $0.003 [1 실행 = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 외부 파이프라인 활동 = $0.000041(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.00025/시간)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>데이터를 복사하고 동적 매개 변수를 사용하여 시간별 변환

이 시나리오에서는 매시간 일정으로 AWS S3에서 Azure Blob Storage로 데이터를 복사하고 Azure Databricks(스크립트의 동적 매개 변수와 함께)를 사용하여 매시간 일정으로 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. AWS S3에서 복사될 데이터에 대한 입력 데이터 세트 및 Azure Storage의 데이터에 대한 출력 데이터 세트가 있는 하나의 복사 작업
2. 변환 스크립트에 매개 변수를 동적으로 전달하는 하나의 조회 작업
3. 데이터 변환에 대한 하나의 Azure Databricks 작업
4. 파이프라인을 1시간마다 실행하는 하나의 일정 트리거

:::image type="content" source="media/pricing-concepts/scenario3.png" alt-text="다이어그램은 일정 트리거가 있는 파이프라인을 보여줍니다. 파이프라인에서 복사 작업은 입력 데이터 세트, 출력 데이터 세트 및 Azure Databricks 실행되는 DataBricks 작업으로 흐르는 조회 작업으로 흐릅니다. 입력 데이터 세트는 A W S3 연결된 서비스로 흐릅니다. 출력 데이터 세트는 Azure Storage 연결된 서비스로 흐릅니다.":::

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 3개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 4개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 3개) |
| 데이터 복사 가정: 실행 시간 = 10분 | 10 \* 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 4개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 3개) |
| 조회 작업 실행 가정: 실행 시간 = 1분 | 1분 파이프라인 활동 실행 |
| Databricks 활동 실행 가정: 실행 시간 = 10분 | 10분 외부 파이프라인 활동 실행 |

**총 시나리오 가격 책정: $0.17020**

- Data Factory 작업 = **$0.00013**
  - 읽기/쓰기 = 11 \* 0.00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 4 \* 0.000005 = $0.00002 [1 모니터링 = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$0.17007**
  - 활동 실행 = 0.001 \* 4 = $0.004 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.166(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 파이프라인 활동 = $0.00003(실행 시간의 1분에 대해 비례합니다. Azure Integration Runtime에서 $0.002/시간)
  - 외부 파이프라인 활동 = $0.000041(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.00025/시간)

## <a name="run-ssis-packages-on-azure-ssis-integration-runtime"></a>Azure에서 SSIS 패키지 실행-SSIS 통합 런타임

Azure-SSIS (SSIS 통합 런타임)는 ADF (Azure Data Factory)에서 SSIS 패키지를 실행 하기 위한 Azure Vm (가상 머신)의 특수 클러스터입니다. 프로 비전 할 때 사용자를 위한 것 이므로, SSIS 패키지를 실행 하는 데 사용 하는지 여부에 관계 없이 계속 실행 되는 동안 다른 전용 Azure Vm과 마찬가지로 요금이 청구 됩니다. 실행 비용을 고려 하 여 ADF 포털의 설정 창에서 시간당 예상치를 확인할 수 있습니다. 예를 들면 다음과 같습니다.  

:::image type="content" source="media/pricing-concepts/ssis-pricing-example.png" alt-text="SSIS 가격 책정 예":::

위의 예제에서 2 시간 동안 Azure-SSIS IR를 실행 하는 경우 **2 (시간) x us $1.158/hour = us $2.316** 요금이 청구 됩니다.

Azure-SSIS IR 실행 비용을 관리 하기 위해 VM 크기를 축소 하 고, 클러스터 크기를 확장 하 고, 상당한 절감 [Azure-SSIS IR](https://azure.microsoft.com/pricing/details/data-factory/ssis/)효과를 제공 하는 Azure 하이브리드 혜택 (AHB)를 통해 사용자 고유의 SQL Server 라이선스를 가져올 수 있습니다 .이 옵션을 사용 하면 편리한/주문형/주문형/SSIS 작업을 처리할 때 언제 든 지 &을 중지할 수 있습니다.  [Azure-SSIS IR 다시 구성](manage-azure-ssis-integration-runtime.md#to-reconfigure-an-azure-ssis-ir) 및 [일정 Azure-SSIS IR](how-to-schedule-azure-ssis-integration-runtime.md)을 참조 하세요.

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>일반 workday에 대한 매핑 데이터 흐름 디버그 사용

데이터 엔지니어인 Sam은 매일 매핑 데이터 흐름의 설계, 빌드 및 테스트를 담당합니다. Sam은 오전에 ADF UI에 로그인하고 데이터 흐름에 대해 디버그 모드를 사용하도록 설정합니다. 디버그 세션에 대한 기본 TTL은 60분입니다. Sam은 8시간 동안 하루 종일 작업하므로 디버그 세션이 만료되지 않습니다. 따라서 해당 날짜에 대한 Sam의 요금은 다음과 같습니다.

**8(시간) x 8(계산에 최적화된 코어) x $0.193 = $12.35**

동시에 다른 데이터 엔지니어인 Chris 또한 데이터 프로파일링 및 ETL 디자인 작업을 위해 ADF 브라우저 UI에 로그인합니다. Chris는 Sam과는 달리 ADF에서 하루 종일 작업을 하지는 않습니다. Chris는 Sam과 동일한 기간 및 동일 날짜에 1시간만 데이터 흐름 디버거를 사용하면 됩니다. 다음은 디버그 사용량과 관련하여 Chris에게 발생한 비용입니다.

**1(시간) x 8(범용 코어) x $0.274 = $2.19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>매핑 데이터 흐름을 사용하여 BLOB 저장소의 데이터 변환

이 시나리오에서는 매시간 일정에 따라 ADF 매핑 데이터 흐름에서 시각적으로 Blob 저장소의 데이터를 변환하려고 합니다.

시나리오를 달성하려면 다음 항목을 사용하여 파이프라인을 만들어야 합니다.

1. 변환 논리를 사용하는 데이터 흐름 활동입니다.

2. Azure Storage의 데이터에 대한 입력 데이터 세트

3. Azure Storage의 데이터에 대한 출력 데이터 세트

4. 파이프라인을 1시간마다 실행하는 일정 트리거

| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 2개의 읽기/쓰기 엔터티  |
| 데이터 세트 생성 | 4개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 2개, 연결된 서비스 참조에 대해 2개) |
| 파이프라인 만들기 | 3개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 1개, 데이터 세트 참조에 대해 2개) |
| 파이프라인 가져오기 | 1개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 2개의 활동 실행(트리거 실행에 대해 1개, 활동 실행에 대해 1개) |
| 데이터 흐름 가정: 실행 시간 = 10분 + 10분 TTL | \*TTL이 10인 일반컴퓨팅의 10 \* 16 코어 |
| 파이프라인 모니터링 가정: 하나의 실행만 발생했습니다. | 다시 시도되는 2개의 모니터링 실행 기록(파이프라인 실행에 대해 1개, 활동 실행에 대해 1개) |

**총 시나리오 가격 책정: $1.4631**

- Data Factory 작업 = **$0.0001**
  - 읽기/쓰기 = 10 \* 0.00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 모니터링 = 2 \* 0.000005 = $0.00001 [1 모니터링 = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 &amp; 실행 = **$1.463**
  - 활동 실행 = 0.001 \* 2 = $0.002 [1 run = $1/1000 = 0.001]
  - 데이터 흐름 활동 = $1.461, 20분(10분 실행 시간 + 10분 TTL)에 비례하여 계산됩니다. 16 코어 일반 컴퓨팅을 사용하는 Azure Integration Runtime의 $0.274/시간

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure Data Factory 관리 VNET의 데이터 통합
이 시나리오에서는 Azure Blob Storage에서 원래 파일을 삭제하고, Azure SQL Database에서 Azure Blob Storage로 데이터를 복사하려고 합니다. 서로 다른 파이프라인에서 이 실행을 두 번 수행합니다. 이러한 두 파이프라인의 실행 시간은 겹칩니다.
:::image type="content" source="media/pricing-concepts/scenario-4.png" alt-text="시나리오 4":::
시나리오를 수행 하려면 다음 항목을 사용 하 여 두 개의 파이프라인을 만들어야 합니다.
  - 파이프라인 활동 – 활동 삭제.
  - Azure Blob storage에서 복사될 데이터에 대한 입력 데이터 세트가 있는 복사 작업
  - Azure SQL Database의 데이터에 대한 출력 데이터 세트
  - 파이프라인을 실행하기 위한 일정 트리거입니다.


| **작업** | **형식 및 단위** |
| --- | --- |
| 연결된 서비스 만들기 | 4개의 읽기/쓰기 엔터티 |
| 데이터 세트 생성 | 8개의 읽기/쓰기 엔터티(데이터 세트 만들기에 대해 4개, 연결된 서비스 참조에 대해 4개) |
| 파이프라인 만들기 | 6개의 읽기/쓰기 엔터티(파이프라인 만들기에 대해 2개, 데이터 세트 참조에 대해 4개) |
| 파이프라인 가져오기 | 2개의 읽기/쓰기 엔터티 |
| 파이프라인 실행 | 6개의 활동 실행(트리거 실행에 대해 2개, 활동 실행에 대해 4개) |
| 활동 삭제 실행: 각 실행 시간은 5분입니다. 첫 번째 파이프라인에서의 활동 삭제 실행은 오전 10:00(UTC)부터 오전 10:05(UTC)까지입니다. 두 번째 파이프라인에서 활동 삭제 실행은 오전 10:02(UTC)부터 오전 10:07(UTC)까지입니다.|관리 VNET에서 총 7분 간 파이프라인 활동 실행. 파이프라인 활동은 관리 VNET에서 최대 50개의 동시 작업을 지원합니다. 파이프라인 작업에 대해 60분 TTL(Time To Live)이 있습니다.|
| 데이터 복사 가정: 각 실행 시간은 10 분입니다. 첫 번째 파이프라인의 복사 실행은 오전 10:06(UTC)부터 오전 10:15(UTC)까지입니다. 두 번째 파이프라인에서 복사 작업 실행은 오전 10:08(UTC)부터 오전 10:17(UTC)까지입니다. | 10 * 4 Azure Integration Runtime(기본 DIU 설정 = 4) 데이터 통합 단위 및 복사 성능 최적화에 대한 자세한 내용은 [이 문서](copy-activity-performance.md)를 참조하세요. |
| 파이프라인 모니터링 가정: 2개의 실행만 발생했습니다. | 다시 시도되는 6개의 모니터링 실행 기록(파이프라인 실행에 대해 2개, 활동 실행에 대해 4개) |


**총 시나리오 가격 책정: $1.45523**

- Data Factory 작업 = $0.00023
  - 읽기/쓰기 = 20 * 0.00001 = $0.0002 [1 R/W = $0.50/50000 = 0.00001]
  - Monitoring = 6 * 0.000005 = $0.00003 [1 Monitoring = $0.25/50000 = 0.000005]
- 파이프라인 오케스트레이션 및 실행 = $1.455
  - 활동 실행 = 0.001 * 6 = $0.006 [1 run = $1/1000 = 0.001]
  - 데이터 이동 활동 = $0.333(실행 시간의 10분에 대해 비례합니다. Azure Integration Runtime에서 $0.25/시간)
  - 파이프라인 작업 = $1.116(7분의 실행 시간에 60분 TTL을 더한 시간에 대해 비례합니다. Azure Integration Runtime에서 $1/시간)

> [!NOTE] 
> 이러한 가격은 설명을 돕기 위한 예시일 뿐입니다.

**FAQ**

Q: 50개 이상의 파이프라인 활동을 실행 하려는 경우, 이러한 활동을 동시에 실행할 수 있습니까?

A: 최대 50개의 동시 파이프라인 작업을 허용합니다.  51번 째 파이프라인 활동은 "빈 슬롯"이 열릴 때까지 큐에 대기합니다. 외부 활동에 대해서도 동일합니다. 최대 800개의 동시 외부 활동이 허용됩니다.

## <a name="next-steps"></a>다음 단계

Azure Data Factory에 대한 가격 책정을 이해했으므로 시작할 수 있습니다.

- [Azure Data Factory UI를 사용하여 데이터 팩터리 만들기](quickstart-create-data-factory-portal.md)

- [Azure Data Factory 소개](introduction.md)

- [Azure Data Factory에서 시각적 작성](author-visually.md)
